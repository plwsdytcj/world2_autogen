from typing import Literal
import os
from urllib.parse import urlparse, quote
from html_to_markdown import convert_to_markdown
import httpx
from bs4 import BeautifulSoup
from bs4.element import Tag as Bs4Tag  # type: ignore


def clean_html(html_content: str) -> str:
    """
    Cleans an HTML string by trying to extract the main content,
    removing unwanted elements and attributes.
    """
    if not html_content:
        return ""

    soup = BeautifulSoup(html_content, "lxml")

    content_selectors = [
        "article",
        "#article",
        ".article",
        "main",
        "#main",
        ".main",
        '[role="main"]',
        "#content",
        ".content",
        ".post",
    ]

    content = None
    for selector in content_selectors:
        selected = soup.select(selector)
        if len(selected) == 1:
            content = selected[0]
            break

    target = content if content else soup.body
    if not target:
        target = soup

    elements_to_remove = [
        "header",
        "footer",
        "nav",
        '[role="navigation"]',
        ".sidebar",
        '[role="complementary"]',
        ".nav",
        ".menu",
        ".header",
        ".footer",
        ".advertisement",
        ".ads",
        ".cookie-notice",
        ".social-share",
        ".related-posts",
        ".comments",
        "#comments",
        ".popup",
        ".modal",
        ".overlay",
        ".banner",
        ".alert",
        ".notification",
        ".subscription",
        ".newsletter",
        ".share-buttons",
        "script",
        "style",
        "noscript",
        "iframe",
        "button",
        "form",
        "input",
        "textarea",
        "select",
        ".noprint",
    ]

    for element in target.select(", ".join(elements_to_remove)):
        element.decompose()

    for html_element in target.find_all(True):
        if isinstance(html_element, Bs4Tag):
            html_element.attrs = {
                key: value
                for key, value in html_element.attrs.items()
                if not key.startswith("on")
                and not key.startswith("aria-")
                and not key.startswith("data-")
                and not key.startswith("role")
                and key not in ["style", "target", "src"]
            }
            # if "src" in html_element.attrs:
            #     src = html_element.attrs["src"]
            #     if isinstance(src, str) and src.startswith("data:"):
            #         html_element.attrs["src"] = "..."

    cleaned_html = target.decode_contents()

    # I'm not sure about this
    # cleaned_html = re.sub(r"[\t\r\n]+", " ", cleaned_html)
    # cleaned_html = re.sub(r"\s{2,}", " ", cleaned_html)
    cleaned_html = cleaned_html.strip()

    return cleaned_html


def html_to_markdown(html_content: str) -> str:
    cleaned_html_str = clean_html(html_content)
    return convert_to_markdown(cleaned_html_str).strip()


class Scraper:
    """A simple scraper to fetch and parse web content."""

    def __init__(self, timeout: int = 10):
        self.timeout = timeout

    async def get_content(
        self,
        url: str,
        type: Literal["html", "markdown"] = "html",
        clean: bool = False,
        pretty: bool = False,
    ) -> str:
        """
        Fetches the content of a URL.
        Returns the HTML content as a string.
        """
        cookies = {"ageVerified": "true"}
        headers = {
            "User-Agent": os.getenv(
                "SCRAPER_USER_AGENT",
                "lorecard/2.5 (+https://github.com/bmen25124/lorecard)",
            ),
            "Accept-Language": os.getenv("SCRAPER_ACCEPT_LANGUAGE", "en-US,en;q=0.9"),
        }

        async with httpx.AsyncClient(follow_redirects=True, headers=headers) as client:
            try:
                response = await client.get(url, timeout=self.timeout, cookies=cookies)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                # Fallback for Wikipedia: try the REST HTML endpoint when blocked
                host = urlparse(url).netloc
                if (
                    e.response is not None
                    and e.response.status_code in (403, 429)
                    and "wikipedia.org" in host
                ):
                    rest_html = await self._fetch_wikipedia_rest_html(client, url)
                    if rest_html:
                        html = rest_html
                        # continue to the post-processing section below
                        content_type = "text/html"
                        # fall through
                    else:
                        raise
                else:
                    raise
            else:
                content_type = response.headers.get("Content-Type", "")
                html = response.text

            if "text/html" not in content_type:
                raise ValueError(f"Invalid content type: {content_type}")

            if clean:
                html = clean_html(html)
            if type == "markdown":
                return html_to_markdown(html)

            if pretty and type == "html":
                html = BeautifulSoup(html, "lxml").prettify()
            return html.strip()

    async def _fetch_wikipedia_rest_html(self, client: httpx.AsyncClient, url: str) -> str | None:
        """Attempt to fetch HTML via Wikimedia REST API for a /wiki/<Title> page.

        Example: https://en.wikipedia.org/wiki/Foobar ->
                 https://en.wikipedia.org/api/rest_v1/page/html/Foobar
        """
        try:
            parsed = urlparse(url)
            if not parsed.path.startswith("/wiki/"):
                return None
            title = parsed.path[len("/wiki/") :]
            if not title:
                return None
            rest_url = f"{parsed.scheme}://{parsed.netloc}/api/rest_v1/page/html/{quote(title)}"
            resp = await client.get(rest_url, timeout=self.timeout)
            resp.raise_for_status()
            if "text/html" in resp.headers.get("Content-Type", ""):
                return resp.text
            return None
        except Exception:
            return None
